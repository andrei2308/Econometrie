influential_points <- which(cooks_dist > threshold)
# Eliminare observații influente
dataset_cleaned <- dataset_2022[-influential_points, ]
model_cleaned <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = dataset_cleaned)
summary(model_cleaned)
# Testul Jarque-Bera
residuals <- resid(model_cleaned)
jb_test <- jarque.bera.test(residuals)
jb_test
#analizam forma distributiei
# Histogramă a reziduurilor
hist(residuals, breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")
# QQ plot
qqnorm(residuals)
qqline(residuals, col = "red")
ols_plot_cooksd_chart(model_cleaned)
#Prognoze
#impartim setul de date
set.seed(123)
train_indices <- sample(1:nrow(dataset_2022), 0.8 * nrow(dataset_2022))
train_data <- dataset_2022[train_indices, ]
test_data <- dataset_2022[-train_indices, ]
# Construirea modelului pe setul de antrenare
model_train <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = train_data)
summary(model_train)
# Prognoze și calcul MAPE
predictions <- predict(model_train, newdata = test_data)
errors <- abs(test_data$`Approx..Total.Revenue.INR.` - predictions)
mape <- mean(errors / test_data$`Approx..Total.Revenue.INR.`) * 100
print(mape)
# Filtrare pentru anul 2022
dataset_2022 <- subset(dataset, format(Date, "%Y") == "2019")
head(dataset_2022)
str(dataset_2022)
# Construirea modelului liniar folosind datele din 2022 (dataset_2022)
model <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = dataset_2022)
# Rezumatul modelului
summary(model)
# Calculul reziduurilor din model
residuals <- resid(model)
# Calculul mediei reziduurilor
mean_residuals <- mean(residuals)
mean_residuals
# Testul Breusch-Pagan pentru homoscedasticitate
bp_test <- bptest(model)
bp_test
# Testul White
white_test <- bptest(model, ~ fitted(model) + I(fitted(model)^2), data = dataset_2022)
white_test
#Pentru a rezolva probleme asociate cu varianta reziduurilor (heteroschedasticitate),
#vom aplica regresia cu valori robuste,
#modelul robust ajusteaza erorile standard, astfel incat sa fie mai precise in cazul
#heteroscedasticitatii
coeftest(model, vcov = vcovHC(model, type = "HC"))
bp_test <- bptest(model)
bp_test
# Testul White
white_test <- bptest(model, ~ fitted(model) + I(fitted(model)^2), data = dataset_2022)
white_test
#Rulam testul Darbin-Watson pentru a testa autocorelarea erorilor
# Testul Durbin-Watson
dw_test <- dwtest(model)
dw_test
#Pentru testarea legaturii dintre regresor si erorile aleatoare vom folosi testul de corelatie in R
residuals <- resid(model) #extragem valorile reziduale
cor_test <- cor.test(dataset_2022$`Quantity.Sold..liters.kg.`, residuals)
cor_test
#aplicam jarque-bera pentru a verifica normalitatea erorilor
# Calculul reziduurilor
residuals <- resid(model)
# Testul Jarque-Bera
jb_test <- jarque.bera.test(residuals)
jb_test
#analizam forma distributiei
# Histogramă a reziduurilor
hist(residuals, breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")
# QQ plot
qqnorm(residuals)
qqline(residuals, col = "red")
ols_plot_cooksd_chart(model)
#incercam refacerea modelului pentru a obtine o forma normal distribuita a reziduurilor
hprice_model <- olsrr::ols_hprice_cook(model)
summary(hprice_model)
# Calculează distanțele Cook
cooks_dist <- cooks.distance(model)
# Pragul pentru observații influente
threshold <- 4 / nrow(dataset_2022)
# Observații influente
influential_points <- which(cooks_dist > threshold)
# Eliminare observații influente
dataset_cleaned <- dataset_2022[-influential_points, ]
model_cleaned <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = dataset_cleaned)
summary(model_cleaned)
# Testul Jarque-Bera
residuals <- resid(model_cleaned)
jb_test <- jarque.bera.test(residuals)
jb_test
#analizam forma distributiei
# Histogramă a reziduurilor
hist(residuals, breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")
# QQ plot
qqnorm(residuals)
qqline(residuals, col = "red")
ols_plot_cooksd_chart(model_cleaned)
#Prognoze
#impartim setul de date
set.seed(123)
train_indices <- sample(1:nrow(dataset_2022), 0.8 * nrow(dataset_2022))
train_data <- dataset_2022[train_indices, ]
test_data <- dataset_2022[-train_indices, ]
# Construirea modelului pe setul de antrenare
model_train <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = train_data)
summary(model_train)
# Prognoze și calcul MAPE
predictions <- predict(model_train, newdata = test_data)
errors <- abs(test_data$`Approx..Total.Revenue.INR.` - predictions)
mape <- mean(errors / test_data$`Approx..Total.Revenue.INR.`) * 100
print(mape)
# Filtrare pentru anul 2022
dataset_2022 <- subset(dataset, format(Date, "%Y") == "2022")
head(dataset_2022)
str(dataset_2022)
# Construirea modelului liniar folosind datele din 2022 (dataset_2022)
model <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = dataset_2022)
# Rezumatul modelului
summary(model)
# Calculul reziduurilor din model
residuals <- resid(model)
# Calculul mediei reziduurilor
mean_residuals <- mean(residuals)
mean_residuals
# Testul Breusch-Pagan pentru homoscedasticitate
bp_test <- bptest(model)
bp_test
# Testul White
white_test <- bptest(model, ~ fitted(model) + I(fitted(model)^2), data = dataset_2022)
white_test
#Pentru a rezolva probleme asociate cu varianta reziduurilor (heteroschedasticitate),
#vom aplica regresia cu valori robuste,
#modelul robust ajusteaza erorile standard, astfel incat sa fie mai precise in cazul
#heteroscedasticitatii
coeftest(model, vcov = vcovHC(model, type = "HC"))
bp_test <- bptest(model)
bp_test
# Testul White
white_test <- bptest(model, ~ fitted(model) + I(fitted(model)^2), data = dataset_2022)
white_test
#Rulam testul Darbin-Watson pentru a testa autocorelarea erorilor
# Testul Durbin-Watson
dw_test <- dwtest(model)
dw_test
#Pentru testarea legaturii dintre regresor si erorile aleatoare vom folosi testul de corelatie in R
residuals <- resid(model) #extragem valorile reziduale
cor_test <- cor.test(dataset_2022$`Quantity.Sold..liters.kg.`, residuals)
cor_test
#aplicam jarque-bera pentru a verifica normalitatea erorilor
# Calculul reziduurilor
residuals <- resid(model)
# Testul Jarque-Bera
jb_test <- jarque.bera.test(residuals)
jb_test
#analizam forma distributiei
# Histogramă a reziduurilor
hist(residuals, breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")
# QQ plot
qqnorm(residuals)
qqline(residuals, col = "red")
ols_plot_cooksd_chart(model)
#incercam refacerea modelului pentru a obtine o forma normal distribuita a reziduurilor
hprice_model <- olsrr::ols_hprice_cook(model)
summary(hprice_model)
# Calculează distanțele Cook
cooks_dist <- cooks.distance(model)
# Pragul pentru observații influente
threshold <- 4 / nrow(dataset_2022)
# Observații influente
influential_points <- which(cooks_dist > threshold)
# Eliminare observații influente
dataset_cleaned <- dataset_2022[-influential_points, ]
model_cleaned <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = dataset_cleaned)
summary(model_cleaned)
# Testul Jarque-Bera
residuals <- resid(model_cleaned)
jb_test <- jarque.bera.test(residuals)
jb_test
#analizam forma distributiei
# Histogramă a reziduurilor
hist(residuals, breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")
# QQ plot
qqnorm(residuals)
qqline(residuals, col = "red")
ols_plot_cooksd_chart(model_cleaned)
#Prognoze
#impartim setul de date
set.seed(123)
train_indices <- sample(1:nrow(dataset_2022), 0.8 * nrow(dataset_2022))
train_data <- dataset_2022[train_indices, ]
test_data <- dataset_2022[-train_indices, ]
# Construirea modelului pe setul de antrenare
model_train <- lm(`Approx..Total.Revenue.INR.` ~ `Quantity.Sold..liters.kg.`, data = train_data)
summary(model_train)
# Prognoze și calcul MAPE
predictions <- predict(model_train, newdata = test_data)
errors <- abs(test_data$`Approx..Total.Revenue.INR.` - predictions)
mape <- mean(errors / test_data$`Approx..Total.Revenue.INR.`) * 100
print(mape)
# Definim valorile pentru prognoză
new_values <- data.frame(`Quantity.Sold..liters.kg.` = c(200, 250, 270, 300, 320))
# Prognozăm valorile folosind modelul curent
predictions <- predict(model, newdata = new_values, se.fit = TRUE, interval = "confidence", level = 0.90)
# Afișăm prognozele și intervalele de încredere
predictions
#cream variabila dummy - vom folosi retail ca si categorie de referinta implicita
dataset_2022$Sales_Channel_Online <- ifelse(dataset_2022$`Sales Channel` == "Online", 1, 0)
colnames(dataset)
#cream variabila dummy - vom folosi retail ca si categorie de referinta implicita
dataset_2022$Sales_Channel_Online <- ifelse(dataset_2022$`Sales.Channel` == "Online", 1, 0)
dataset_2022$Sales_Channel_Wholesale <- ifelse(dataset_2022$`Sales.Channel` == "Wholesale", 1, 0)
# Construirea modelului de regresie multiplă
model_multiple <- lm(`Approx. Total Revenue(INR)` ~
`Quantity Sold (liters/kg)` +
`Price per Unit (sold)` +
Sales_Channel_Online +
Sales_Channel_Wholesale, data = dataset_2022)
# Construirea modelului de regresie multiplă
model_multiple <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
Sales_Channel_Online +
Sales_Channel_Wholesale, data = dataset_2022)
# Rezumatul modelului
summary(model_multiple)
colnames(dataset)
#cream variabila dummy - vom folosi retail ca si categorie de referinta implicita
# Creare variabile dummy pentru Storage Condition
dataset_2022$Storage_Frozen <- ifelse(dataset_2022$`Storage.Condition` == "Frozen", 1, 0)
dataset_2022$Storage_TetraPack <- ifelse(dataset_2022$`Storage.Condition` == "Tetra Pack", 1, 0)
dataset_2022$Storage_Refrigerated <- ifelse(dataset_2022$`Storage.Condition` == "Refrigerated", 1, 0)
dataset_2022$Storage_Polythene <- ifelse(dataset_2022$`Storage.Condition` == "Polythene Packet", 1, 0)
# Construirea modelului de regresie multiplă
model_multiple <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
Storage_Frozen +
Storage_TetraPack +
Storage_Refrigerated +
Storage_Polythene, data = dataset_2022)
# Rezumatul modelului
summary(model_multiple)
#cream variabila dummy - vom folosi retail ca si categorie de referinta implicita
# Creare variabile dummy pentru Storage Condition
# Creare variabile dummy pentru Farm Size
dataset_2022$Farm_Size_Medium <- ifelse(dataset_2022$`Farm.Size` == "Medium", 1, 0)
dataset_2022$Farm_Size_Large <- ifelse(dataset_2022$`Farm.Size` == "Large", 1, 0)
# Construirea modelului de regresie multiplă
model_multiple <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
Farm_Size_Medium +
Farm_Size_Large, data = dataset_2022)
# Rezumatul modelului
summary(model_multiple)
# Construirea modelului de regresie multiplă
# Construirea modelului de regresie multiplă
model_multiple <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
Farm_Size_Medium +
Farm_Size_Large, data = dataset_2022)
# Rezumatul modelului
summary(model_multiple)
#cream variabila dummy - vom folosi retail ca si categorie de referinta implicita
dataset_2022$Sales_Channel_Online <- ifelse(dataset_2022$`Sales.Channel` == "Online", 1, 0)
dataset_2022$Sales_Channel_Wholesale <- ifelse(dataset_2022$`Sales.Channel` == "Wholesale", 1, 0)
# Construirea modelului de regresie multiplă
model_multiple <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
Sales_Channel_Online +
Sales_Channel_Wholesale, data = dataset_2022)
# Rezumatul modelului
summary(model_multiple)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number of Cows` >= 50, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 50, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 30, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 20, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 70, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 60, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 65, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 68, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 78, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 100, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 200, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 11, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 50, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 60, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 70, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
# Creare variabilă dummy pentru Number of Cows
dataset_2022$High_Number_Cows <- ifelse(dataset_2022$`Number.of.Cows` >= 68, 1, 0)
# Construirea modelului de regresie multiplă folosind doar High_Number_Cows
model_single_dummy <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_2022)
# Rezumatul modelului
summary(model_single_dummy)
#Forma functionala este liniara
plot(fitted(model_single_dummy), resid(model_single_dummy),
main = "Reziduuri vs Valori ajustate",
xlab = "Valori ajustate", ylab = "Reziduuri")
abline(h = 0, col = "red")
summary(dataset_2022$`Quantity.Sold..liters.kg.`)
summary(dataset_2022$`Price.per.Unit..sold.`)
table(dataset_2022$High_Number_Cows)
#Homoschedasticitatea erorilor aleatoare
bptest(model_single_dummy)
bptest(model_single_dummy, ~ fitted(model_single_dummy) + I(fitted(model_single_dummy)^2), data = dataset_2022)
#avem heteroschedasticitate confirmata de ambele teste => aplicam metoda regresiilor cu erori robuste
coeftest(model_single_dummy, vcov = vcovHC(model_single_dummy, type = "HC"))
#Erorile nu sunt autocorelate
dwtest(model_single_dummy)
#Necorelare intre regresor si erorile aleatoare
cor.test(dataset_2022$`Quantity.Sold..liters.kg.`, resid(model_single_dummy))
cor.test(dataset_2022$`Price.per.Unit..sold.`, resid(model_single_dummy))
cor.test(dataset_2022$High_Number_Cows, resid(model_single_dummy))
#p=1,iar coeficientii de corelatie sunt aproape 0, ceea ce confirma ipoteza
#Erorile au distributie normala
jarque.bera.test(resid(model_single_dummy))
#analiza grafica
hist(resid(model_single_dummy), breaks = 30, main = "Histogramă a Reziduurilor")
qqnorm(resid(model_single_dummy))
qqline(resid(model_single_dummy), col = "red")
#Multicoliniaritate
vif(model_single_dummy)
#erorile nu au distributie normala => identificam valorile extreme si le eliminam
cooks_distance <- cooks.distance(model_single_dummy)
plot(cooks_distance, main = "Cook's Distance")
abline(h = 4 / nrow(dataset_2022), col = "red")
# Pragul pentru identificarea valorilor influente
threshold <- 4 / nrow(dataset_2022)
# Observații influente
influential_points <- which(cooks_distance > threshold)
# Eliminare observații influente din setul de date
dataset_cleaned <- dataset_2022[-influential_points, ]
# Reconstruirea modelului fără observațiile influente
model_cleaned <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = dataset_cleaned)
# Rezumatul noului model
summary(model_cleaned)
# Testăm din nou normalitatea erorilor
residuals_cleaned <- resid(model_cleaned)
jb_test_cleaned <- jarque.bera.test(residuals_cleaned)
jb_test_cleaned
# Analizăm distribuția grafică a reziduurilor
hist(residuals_cleaned, breaks = 30, main = "Histogramă a Reziduurilor (fără valori influente)", xlab = "Reziduuri")
qqnorm(residuals_cleaned)
qqline(residuals_cleaned, col = "red")
install.packages("car")
library(car)
#multicoliniaritate
# Calculează VIF pentru fiecare variabilă independentă
vif_values <- vif(model_cleaned)
# Afișează VIF-urile
print(vif_values)
# toate valorile sunt apropiate de 1 => lipsa multicoliniaritatii
# Împărțim setul de date în set de antrenare și set de testare
set.seed(123)  # Pentru reproducibilitate
train_indices <- sample(1:nrow(dataset_cleaned), 0.8 * nrow(dataset_cleaned))
train_data <- dataset_cleaned[train_indices, ]
test_data <- dataset_cleaned[-train_indices, ]
# Construirea modelului pe setul de antrenare
model_train <- lm(`Approx..Total.Revenue.INR.` ~
`Quantity.Sold..liters.kg.` +
`Price.per.Unit..sold.` +
High_Number_Cows, data = train_data)
# Prognoze pe setul de testare
predictions <- predict(model_train, newdata = test_data)
# Calcularea erorilor absolute
errors <- abs(test_data$`Approx..Total.Revenue.INR.` - predictions)
# Calculul MAPE
mape <- mean(errors / test_data$`Approx..Total.Revenue.INR.`) * 100
print(paste("MAPE:", round(mape, 2), "%"))
# Crearea unui interval de încredere pentru noi valori
new_values <- data.frame(
`Quantity.Sold..liters.kg.` = c(200, 250, 300),
`Price.per.Unit..sold.` = c(50, 55, 60),
High_Number_Cows = c(1, 0, 1)
)
# Prognoze pentru noile valori
predictions_new <- predict(model_train, newdata = new_values, se.fit = TRUE, interval = "confidence", level = 0.90)
# Afișarea prognozelor și a intervalelor de încredere
predictions_new$fit
